{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eefd9d6",
      "metadata": {
        "id": "9eefd9d6",
        "outputId": "94ffbb75-015e-4ca1-b244-86b230fc49f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Toshiba\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "C:\\Users\\Toshiba\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
            "C:\\Users\\Toshiba\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d141d3",
      "metadata": {
        "id": "e1d141d3"
      },
      "outputs": [],
      "source": [
        "translate = {\"cane\": \"Dog\", \"cavallo\": \"Horse\", \"elefante\": \"Elephant\", \"farfalla\": \"Butterfly\", \"gallina\": \"Chicken\", \n",
        "\"gatto\": \"Cat\", \"mucca\": \"Cow\", \"pecora\": \"Sheep\", \"scoiattolo\": \"Squirrel\", \"ragno\": \"Spider\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0624c227",
      "metadata": {
        "id": "0624c227"
      },
      "outputs": [],
      "source": [
        "folders = os.listdir(\"raw-img/\")\n",
        "\n",
        "def resize_gray():\n",
        "    files, categories = [], []\n",
        "    for i, folder in enumerate(folders):\n",
        "        filenames = os.listdir(\"raw-img/\" + folder) # iterate over files\n",
        "        for file in filenames:\n",
        "\n",
        "            im = Image.open(\"raw-img/\" + folder + \"/\" + file)\n",
        "            im = im.convert('L')#convert to gray scale\n",
        "            im = im.resize((100,100)) # resize image\n",
        "            data = np.asarray(im) # convert numpy array\n",
        "            data = data/255.0 # normalize\n",
        "            \n",
        "            sample_row = list() # temp pixel holder list\n",
        "            \n",
        "            for i in data: # for every row\n",
        "                for z in range(len(i)): # for every column\n",
        "                    sample_row.append(i[z]) # append temp list so array is converted from 2D to 1D\n",
        "            \n",
        "            files.append(sample_row)\n",
        "            categories.append(translate[folder])\n",
        "            \n",
        "    return files, categories\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24059e5a",
      "metadata": {
        "id": "24059e5a"
      },
      "outputs": [],
      "source": [
        "files, categories = resize_gray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e814dda7",
      "metadata": {
        "id": "e814dda7",
        "outputId": "a342a990-228f-4651-c16b-caa647a07cb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(26179, 10000)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files_arr = np.array(files)\n",
        "categories_arr = np.array(categories)\n",
        "files_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd774d3",
      "metadata": {
        "id": "cbd774d3"
      },
      "outputs": [],
      "source": [
        "def categories_tonum(category_arr): # convert category arrays element from text to related class number\n",
        "    categories_index = {'Dog' : 0, 'Horse' : 1, 'Elephant' : 2, 'Butterfly' : 3, 'Chicken' : 4, 'Cat' : 5, 'Cow' : 6, \n",
        "                        'Sheep' : 7, 'Spider' : 8, 'Squirrel' : 9}\n",
        "\n",
        "    for i in range(len(category_arr)):\n",
        "        category_arr[i] = categories_index[category_arr[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e26f90a",
      "metadata": {
        "id": "9e26f90a",
        "outputId": "f6c21d73-22ad-498d-f1e5-62a2449b1c30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 9, 9, 9])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories_tonum(categories_arr)\n",
        "categories_arr = categories_arr.astype(np.int)\n",
        "categories_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69ea24f",
      "metadata": {
        "id": "e69ea24f"
      },
      "outputs": [],
      "source": [
        "def shuffle_split(X_arr, Y_arr): # shuffle X and Y by same random seed number so the connection preserved\n",
        "    random.Random(85).shuffle(X_arr)\n",
        "    random.Random(85).shuffle(Y_arr)\n",
        "    return train_test_split(X_arr, Y_arr, test_size=0.20, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339ca5ab",
      "metadata": {
        "id": "339ca5ab",
        "outputId": "8c443966-4044-45a2-80dc-0afed9e5c664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20943, 10000)\n"
          ]
        }
      ],
      "source": [
        "files_arr_train, files_arr_test, categories_arr_train, categories_arr_test = shuffle_split(files_arr, categories_arr)\n",
        "print(files_arr_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d59332",
      "metadata": {
        "id": "e9d59332"
      },
      "outputs": [],
      "source": [
        "def init_params(layers):\n",
        "    #Layers : List consisting of how many nodes each layer has including input and output.\n",
        "    np.random.seed(42)\n",
        "\n",
        "    N = len(layers) #Number of layers in the network.\n",
        "    weights = {}\n",
        "    bias = {}\n",
        "    for i in range(1,N):\n",
        "        weights['W' + str(i)] = np.random.rand(layers[i],layers[i-1])*0.001 # initialize weights randomly and minimize them\n",
        "        bias['b' + str(i)] = np.zeros(layers[i]).reshape(layers[i],1) # initialize biases as 0s\n",
        "\n",
        "    return weights, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7ac6e7",
      "metadata": {
        "id": "df7ac6e7",
        "outputId": "f7a44bd7-9779-4df5-f2bf-d87eef9374fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'W1': array([[3.74540119e-04, 9.50714306e-04, 7.31993942e-04],\n",
            "       [5.98658484e-04, 1.56018640e-04, 1.55994520e-04],\n",
            "       [5.80836122e-05, 8.66176146e-04, 6.01115012e-04]])}, {'b1': array([[0.],\n",
            "       [0.],\n",
            "       [0.]])})\n"
          ]
        }
      ],
      "source": [
        "print(init_params([3,3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e513f8",
      "metadata": {
        "id": "26e513f8"
      },
      "outputs": [],
      "source": [
        "def get_node_num(X, layer_num): # calculate neuron_size of each layer\n",
        "    layer_nodes = list()\n",
        "    layer_nodes.append(len(X[0].T)) # 100x100 pixel images(input layer)\n",
        "    for i in range(layer_num):\n",
        "        neuron_size = int(np.round(len(X)/(2*(layer_nodes[i]/5 + 10))))\n",
        "        layer_nodes.append(neuron_size)\n",
        "    layer_nodes.append(10) # append output layer\n",
        "    return layer_nodes # return list that contains neuron size of each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be54161e",
      "metadata": {
        "id": "be54161e",
        "outputId": "cde46f4e-14e7-4457-bc65-e03dafc52086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10000, 7, 10]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_node_num(files_arr, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965a9b67",
      "metadata": {
        "id": "965a9b67"
      },
      "outputs": [],
      "source": [
        "def RELU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def RELU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def TANH_deriv(x):\n",
        "    return (1 - np.power(np.tanh(x), 2))\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z)\n",
        "    return expZ / np.sum(expZ, axis = 0)\n",
        "\n",
        "def cost_func(A, Y):\n",
        "    sum = 0\n",
        "    predicts = A.T # predictions\n",
        "    \n",
        "    for i in range(len(Y)):\n",
        "        sum += -np.log10(predicts[i][Y[i]]) # sum of negative log likelihood of correct class\n",
        "    sum = sum / len(Y)\n",
        "    return sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294d17d2",
      "metadata": {
        "id": "294d17d2"
      },
      "outputs": [],
      "source": [
        "def forward_prop(weights, bias, X, act_func):\n",
        "    \n",
        "    Z_dict = {}\n",
        "    A_dict = {}\n",
        "    \n",
        "    weight_list = list(weights.keys()) # get keys as list\n",
        "    \n",
        "    for i in range(1, len(weight_list)+1):\n",
        "        Z, A = 0,0\n",
        "        if(i == len(weight_list)): # means output layer\n",
        "            if(i==1): # input layer (the case no hidden layer)\n",
        "                Z = np.dot(weights['W'+str(i)], X.T) + bias['b'+str(i)] # X*W + B\n",
        "                A = softmax(Z)\n",
        "            else: # not input layer\n",
        "                Z = np.dot(weights['W'+str(i)], A_dict['A'+str(i-1)]) + bias['b'+str(i)]\n",
        "                A = softmax(Z)\n",
        "        else: # not output layer\n",
        "            if i == 1: # input layer\n",
        "                Z = np.dot(weights['W'+str(i)], X.T) + bias['b'+str(i)]\n",
        "                if(act_func == 'relu'): # check for activation function\n",
        "                    A = RELU(Z)\n",
        "                else:\n",
        "                    A = tanh(Z)\n",
        "            else:\n",
        "                Z = np.dot(weights['W'+str(i)], A_dict['A'+str(i-1)]) + bias['b'+str(i)]\n",
        "                if(act_func == 'relu'): # check for activation funcion\n",
        "                    A = RELU(Z)\n",
        "                else:\n",
        "                    A = tanh(Z)\n",
        "            \n",
        "        Z_dict['Z'+str(i)] = Z # save calculations into dict\n",
        "        A_dict['A'+str(i)] = A # save calculations into dict\n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    return Z_dict, A_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0ae508",
      "metadata": {
        "id": "3c0ae508"
      },
      "outputs": [],
      "source": [
        "def one_hot(target_list): # return a list that only target class index is 1 and other are 0\n",
        "    one_hot_list = list()\n",
        "    for i in range(len(target_list)):\n",
        "        arr_encoded = [0] * 10 # initialize a list filled with 0's\n",
        "        arr_encoded[target_list[i]] = 1 # set target class index to 1 \n",
        "        one_hot_list.append(arr_encoded)\n",
        "    \n",
        "    one_hot_list = np.array(one_hot_list).T\n",
        "    return one_hot_list # a sample is [0,0,0,1,0,0,0,0,0,0] for target 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4421824",
      "metadata": {
        "id": "c4421824"
      },
      "outputs": [],
      "source": [
        "def back_prop(Z, A, target_list, X, weights, act_func):\n",
        "    \n",
        "    dW_dict = {}\n",
        "    db_dict = {}\n",
        "    dZ_dict = {}\n",
        "    \n",
        "    \n",
        "    one_hot_arr = one_hot(target_list)\n",
        "    \n",
        "    weight_list = list(weights.keys()) # get keys as list\n",
        "    \n",
        "    for i in range(len(weight_list), 0, -1):\n",
        "        dZ, dW, db = 0,0,0\n",
        "        if(i == len(weight_list)): # means output layer\n",
        "            if(i==1): # means input layer (case no hidden layer)\n",
        "                dZ = A['A'+str(i)] - one_hot_arr\n",
        "                dW = np.dot(dZ,X) / len(X)\n",
        "                db = np.sum(dZ, axis=1, keepdims=True) / len(X)\n",
        "            else: # not input layer\n",
        "                dZ = A['A'+str(i)] - one_hot_arr\n",
        "                dW = np.dot(dZ, A['A'+str(i-1)].T) / len(X)\n",
        "                db = np.sum(dZ, axis=1,keepdims=True) / len(X)\n",
        "        else: # not output layer\n",
        "            if(i==1): # means input layer\n",
        "                if(act_func == 'relu'): # check for activation func\n",
        "                    dZ = np.dot(weights['W'+str(i+1)].T, dZ_dict['dZ'+str(i+1)]) * RELU_deriv(Z['Z'+str(i)])\n",
        "                else:\n",
        "                    dZ = np.dot(weights['W'+str(i+1)].T, dZ_dict['dZ'+str(i+1)]) * TANH_deriv(Z['Z'+str(i)])\n",
        "                dW = np.dot(dZ, X) / len(X)\n",
        "                db = np.sum(dZ, axis=1, keepdims=True) / len(X)\n",
        "            else: # not input layer\n",
        "                if(act_func == 'relu'): # check for activation func\n",
        "                    dZ = np.dot(weights['W'+str(i+1)].T, dZ_dict['dZ'+str(i+1)]) * RELU_deriv(Z['Z'+str(i)])\n",
        "                else:\n",
        "                    dZ = np.dot(weights['W'+str(i+1)].T, dZ_dict['dZ'+str(i+1)]) * TANH_deriv(Z['Z'+str(i)]) \n",
        "                dW = np.dot(dZ, A['A'+str(i-1)].T) / len(X)\n",
        "                db = np.sum(dZ, axis=1, keepdims=True) / len(X)\n",
        "        dZ_dict['dZ'+str(i)] = dZ # save calculations\n",
        "        dW_dict['dW'+str(i)] = dW # save calculations\n",
        "        db_dict['db'+str(i)] = db # save calculations\n",
        "    \n",
        "            \n",
        "    return dW_dict, db_dict\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ee1f69",
      "metadata": {
        "id": "67ee1f69"
      },
      "outputs": [],
      "source": [
        "def update_params(weights, bias, dW_dict, db_dict, learn_rate): # no comment is necessary, this function is pretty simple\n",
        "    \n",
        "    for i in range(1, len(weights.keys())+1):\n",
        "        \n",
        "        weights['W'+str(i)] = weights['W'+str(i)] - (learn_rate*dW_dict['dW'+str(i)])\n",
        "        bias['b'+str(i)] = bias['b'+str(i)] - (learn_rate*db_dict['db'+str(i)])\n",
        "        \n",
        "        \n",
        "    \n",
        "    return weights,bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "155bf541",
      "metadata": {
        "id": "155bf541"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(predictions, Y): # get predictions and Y's\n",
        "    total = 0\n",
        "    \n",
        "    for i in range(len(predictions)):\n",
        "        if (predictions[i] == Y[i]): # if correct \n",
        "            total += 1 # increment total by 1\n",
        "    \n",
        "    return total/Y.size # return the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c536e3c",
      "metadata": {
        "id": "7c536e3c"
      },
      "outputs": [],
      "source": [
        "def get_prediction(A): # A is an array which holds lists with shape of 10,1 shape\n",
        "    return np.argmax(A, 0) # the index of max is our prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7648e841",
      "metadata": {
        "id": "7648e841"
      },
      "outputs": [],
      "source": [
        "epoch_list = list()\n",
        "train_cost_list = list()\n",
        "test_cost_list = list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f53ee68",
      "metadata": {
        "id": "8f53ee68"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X_train, Y_train, X_test, Y_test, iterations, learning_rate, hidden_layer_num, mini_batch_size, act_func):\n",
        "    data_len = len(Y_train)\n",
        "    \n",
        "    weights, bias = init_params(get_node_num(X_train, hidden_layer_num)) # initialize parameters\n",
        "    print('Neural Network Created With Nodes:')\n",
        "    print(get_node_num(X_train, hidden_layer_num))\n",
        "    \n",
        "    ### INITIAL ACCURACY AND INITIAL LOSS FOR TRAIN\n",
        "    \n",
        "    z, prediction_list = forward_prop(weights, bias, X_train, act_func) # initial situation\n",
        "    prediction_list = prediction_list['A'+str(2+hidden_layer_num-1)] # initial predictions\n",
        "    \n",
        "    print('Initial:')\n",
        "    cost = cost_func(prediction_list, Y_train) # initial cost\n",
        "    train_cost_list.append(cost)\n",
        "    epoch_list.append(0)\n",
        "    predictions = get_prediction(prediction_list)\n",
        "    print(\"Accuracy: \", get_accuracy(predictions, Y_train)) # initial accuracy\n",
        "    print(cost)\n",
        "    \n",
        "    ### INITIAL ACCURACY AND INITIAL LOSS FOR TEST\n",
        "    \n",
        "    z, prediction_list = forward_prop(weights, bias, X_test, act_func)\n",
        "    prediction_list = prediction_list['A'+str(2+hidden_layer_num-1)]\n",
        "\n",
        "    test_cost_list.append(cost_func(prediction_list, Y_test))\n",
        "    \n",
        "    \n",
        "    #### STARTS LEARNING\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for i in range(iterations):\n",
        "        counter_iter_batch = 0\n",
        "        counter = 0\n",
        "        while(counter+mini_batch_size < data_len):\n",
        "            \n",
        "            Z, A = forward_prop(weights, bias, X_train[counter:counter+mini_batch_size], act_func)\n",
        "            dW, db = back_prop(Z, A, Y_train[counter:counter+mini_batch_size], X_train[counter:counter+mini_batch_size], weights, act_func)\n",
        "            weights, bias = update_params(weights, bias, dW, db, learning_rate)\n",
        "\n",
        "\n",
        "            counter += mini_batch_size\n",
        "            counter_iter_batch += 1\n",
        "        \n",
        "        if(i%10 == 0 and i!=0):\n",
        "            epoch_list.append(i)\n",
        "            \n",
        "            z, prediction_list = forward_prop(weights, bias, X_train, act_func)\n",
        "            prediction_list = prediction_list['A'+str(2+hidden_layer_num-1)]\n",
        "            \n",
        "            print('In epoch ' + str(i) + ':')\n",
        "            sum = cost_func(prediction_list, Y_train)\n",
        "            train_cost_list.append(sum)\n",
        "            predictions = get_prediction(prediction_list)\n",
        "            print(\"Accuracy: \", get_accuracy(predictions, Y_train))\n",
        "            print(sum)\n",
        "            \n",
        "            z, prediction_list = forward_prop(weights, bias, X_test, act_func)\n",
        "            prediction_list = prediction_list['A'+str(2+hidden_layer_num-1)]\n",
        "            predicted_list = get_prediction(prediction_list)\n",
        "            test_cost_list.append(cost_func(prediction_list, Y_test))\n",
        "            \n",
        "    end_time = time.time()\n",
        "    print('\\n Computation time: ' + str(end_time - start_time))\n",
        "    return weights, bias, sum, predicted_list, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c293cb7",
      "metadata": {
        "id": "4c293cb7",
        "outputId": "9ab3773a-2006-4e5c-edfa-c665d53bc1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network Created With Nodes:\n",
            "[10000, 10]\n",
            "Initial:\n",
            "Accuracy:  0.09750274554743829\n",
            "1.0011443151384576\n",
            "In epoch 10:\n",
            "Accuracy:  0.16592656257460728\n",
            "1.071417789034592\n",
            "In epoch 20:\n",
            "Accuracy:  0.16740677075872606\n",
            "1.0967433293010787\n",
            "In epoch 30:\n",
            "Accuracy:  0.19863438857852267\n",
            "0.9750385107840105\n",
            "In epoch 40:\n",
            "Accuracy:  0.2556462779926467\n",
            "1.0410121562901213\n",
            "In epoch 50:\n",
            "Accuracy:  0.22160148975791433\n",
            "1.115942596809649\n",
            "In epoch 60:\n",
            "Accuracy:  0.20885259991405242\n",
            "1.0964426391532471\n",
            "In epoch 70:\n",
            "Accuracy:  0.24910471279186364\n",
            "1.197646786554083\n",
            "In epoch 80:\n",
            "Accuracy:  0.20617867545241847\n",
            "1.350673634444734\n",
            "In epoch 90:\n",
            "Accuracy:  0.25001193716277514\n",
            "1.0936042741613867\n",
            "In epoch 100:\n",
            "Accuracy:  0.22766556844769134\n",
            "0.9738301025217836\n",
            "In epoch 110:\n",
            "Accuracy:  0.26777443537220075\n",
            "0.9296628032846439\n",
            "In epoch 120:\n",
            "Accuracy:  0.28486845246621784\n",
            "0.9126198499742213\n",
            "In epoch 130:\n",
            "Accuracy:  0.2675356921166977\n",
            "1.0129569744598383\n",
            "In epoch 140:\n",
            "Accuracy:  0.3062120995081889\n",
            "0.861576743030091\n",
            "In epoch 150:\n",
            "Accuracy:  0.2415126772668672\n",
            "0.9656818455928505\n",
            "In epoch 160:\n",
            "Accuracy:  0.3089337726209235\n",
            "0.8840166379279625\n",
            "In epoch 170:\n",
            "Accuracy:  0.2914577663181015\n",
            "0.9642193363743328\n",
            "In epoch 180:\n",
            "Accuracy:  0.2845342119085136\n",
            "1.031787712859774\n",
            "In epoch 190:\n",
            "Accuracy:  0.29160101227140334\n",
            "0.9120614012867407\n",
            "In epoch 200:\n",
            "Accuracy:  0.32173041111588596\n",
            "0.8400475974991775\n",
            "In epoch 210:\n",
            "Accuracy:  0.31571408107720955\n",
            "0.9047234890079198\n",
            "In epoch 220:\n",
            "Accuracy:  0.3002912667717137\n",
            "0.9884258090182878\n",
            "In epoch 230:\n",
            "Accuracy:  0.30740581578570403\n",
            "0.8820397505521542\n",
            "In epoch 240:\n",
            "Accuracy:  0.3182447595855417\n",
            "0.889486344596734\n",
            "In epoch 250:\n",
            "Accuracy:  0.2824810199111875\n",
            "0.9248588689458109\n",
            "In epoch 260:\n",
            "Accuracy:  0.34302630950675644\n",
            "0.8290519790181802\n",
            "In epoch 270:\n",
            "Accuracy:  0.28634866065033665\n",
            "1.0396867853306064\n",
            "In epoch 280:\n",
            "Accuracy:  0.32364035715991024\n",
            "0.879182971163676\n",
            "In epoch 290:\n",
            "Accuracy:  0.3328558468223273\n",
            "0.8216177535131796\n",
            "\n",
            " Computation time: 282.58590960502625\n"
          ]
        }
      ],
      "source": [
        "weights, bias, sum, predictions, Y_test = gradient_descent(files_arr_train, categories_arr_train, files_arr_test, categories_arr_test, 300, 0.008, 1, 128, 'relu')\n",
        "np.save('weights.npy', weights)\n",
        "np.save('bias.npy', bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report"
      ],
      "metadata": {
        "id": "pkiPVeX1vG6C"
      },
      "id": "pkiPVeX1vG6C"
    },
    {
      "cell_type": "markdown",
      "id": "d968b629",
      "metadata": {
        "id": "d968b629"
      },
      "source": [
        "## Comment on different parameters of  0 hidden layers model\n",
        "\n",
        "| Accuracy | batch _size = 16 | batch_size=128 | no_batch_size |\n",
        "| :- | :-: | :-: | :-: |\n",
        "| epoch 0 | 0.097 | 0.097 | 0.097 |\n",
        "| epoch 10 | 0.182 | 0.165 | 0.187 |\n",
        "| epoch 100 | 0.301 | 0.227 | 0.185 |\n",
        "| epoch 300 | 0.370 | 0.350 | 0.187 |\n",
        "\n",
        "\n",
        "| Cost | batch _size = 16 | batch_size=128 | no_batch_size |\n",
        "| :- | :-: | :-: | :-: |\n",
        "| epoch 0 | 1.00 | 1.00 | 1.00 |\n",
        "| epoch 10 | 2.490 | 1.07 | 0.99 |\n",
        "| epoch 100 | 1.160 | 0.97 | 1.11 |\n",
        "| epoch 300 | 0.930 | 0.80 | 1.09 |\n",
        "\n",
        "Computation time for mini_batch_size = 128 is 240.71 secs\n",
        "Computation time for mini_batch_size = 128 is 367.58 secs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cfe0cd5",
      "metadata": {
        "id": "2cfe0cd5"
      },
      "source": [
        "## Different parameters of  1 hidden layers model\n",
        "\n",
        "Same parameters in both cases: Relu, batch size = 128, learning rate = 0.008\n",
        "\n",
        "__Case: neuron size = 5__ in hidden layer\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.070 | 1.00 |\n",
        "| epoch 10 | 0.188 | 0.96 |\n",
        "| epoch 100 | 0.214 | 0.93 |\n",
        "\n",
        "__Case: neuron size = 1000__ in hidden layer\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.070 | 1.00 |\n",
        "| epoch 10 | 0.192 | 0.95 |\n",
        "| epoch 100 | 0.245 | 0.91 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19189fc",
      "metadata": {
        "id": "c19189fc"
      },
      "source": [
        "## Comment on different parameters of  2 hidden layers model\n",
        "\n",
        "Same parameters: hidden layer size = 2, learning rate = 0.008, batch_size = 128 \n",
        "\n",
        "__Case: Relu__\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.187 | 1.00 |\n",
        "| epoch 10 | 0.187 | 0.95 |\n",
        "| epoch 100 | 0.200 | 0.95 |\n",
        "| epoch 300 | 0.237 | 0.93 |\n",
        "\n",
        "__Case: Tanh__\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.187 | 1.00 |\n",
        "| epoch 10 | 0.187 | 0.95 |\n",
        "| epoch 100 | 0.196 | 0.95 |\n",
        "| epoch 300 | 0.201 | 0.95 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5dc292",
      "metadata": {
        "id": "1c5dc292"
      },
      "source": [
        "## Parameters effects on different values\n",
        "\n",
        "### Learning Rate\n",
        "\n",
        "I've runned the model twice, first time; both with 1 hidden layer, RELU function, 60 mini batch size but the learning rate is 0.005 in first run and 0.02 in second run, and the results are:\n",
        "\n",
        "__Learning Rate = 0.005__ / batch_size = 128 / 1 hidden layer / Relu function\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.070 | 1.00 |\n",
        "| epoch 10 | 0.187 | 0.96 |\n",
        "| epoch 100 | 0.214 | 0.94 |\n",
        "| epoch 300 | 0.252 | 0.908 |\n",
        "\n",
        "__Learning Rate = 0.02__ / batch_size = 128 / 1 hidden layer / Relu function\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.070 | 1.00 |\n",
        "| epoch 10 | 0.199 | 0.95 |\n",
        "| epoch 100 | 0.23 | 0.93 |\n",
        "| epoch 300 | 0.24 | 0.93 |\n",
        "\n",
        "In the tables above, the initial(epoch 0) values are same; accuracy is 0.070 in both and loss is 1.00 in both but in 10th epoch accuracy is 0.187 in first table but 0.199 in second also cost is 0.96 in first but 0.95 in second __so we can say that as learning rate increases the model learns faster.__ But if we check the 300th epoch the accuracy of first table is bigger __so if learning rate is big then the final accuracy will be smaller because it will be hard for the model to detect local minimum of cost graph.__\n",
        "\n",
        "\n",
        "### Batch Size\n",
        "\n",
        "Tables from 0 hidden layer part shows us =>\n",
        "\n",
        "First of all using a mini_batch decreases the computation time. Computation time for even the biggest mini_batch_size(128) is 240.71 seconds for 300 epochs while 367.58 seconds for 300 epoch with no mini_batch.\n",
        "\n",
        "Also as we can see accuracy decreases as we don't use batch_size and cost slightly increases.\n",
        "\n",
        "### Layer Size\n",
        "\n",
        "Same parameters: Relu, batch size = 128, learning rate = 0.008\n",
        "\n",
        "no hidden layer\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.097 | 1.00 |\n",
        "| epoch 10 | 0.165 | 1.07 |\n",
        "| epoch 100 | 0.227 | 0.97 |\n",
        "\n",
        "1 hidden layer\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.070 | 1.00 |\n",
        "| epoch 10 | 0.188 | 0.96 |\n",
        "| epoch 100 | 0.214 | 0.93 |\n",
        "\n",
        "2 hidden layer\n",
        "\n",
        "| epoch | Accuracy | Cost |\n",
        "| :- | :-: | :-: |\n",
        "| epoch 0 | 0.187 | 1.00 |\n",
        "| epoch 10 | 0.187 | 0.96 |\n",
        "| epoch 100 | 0.200 | 0.95 |\n",
        "\n",
        "\n",
        "1 hidden layer neural network has smaller loss than no  hidden layer, also it starts with lower accuracy but at 10th epoch it has higher accuracy. 2 hidden layer does not have higher accuracy or lower loss than 1 hidden layer neural network, so nothing clear to say.\n",
        "\n",
        "\n",
        "### Hidden neuron Size\n",
        "\n",
        "In this part I've take the result table from 1 hidden layer part. There are 2 tables with all same parameters but only difference is the neuron size, in first case neuron size is 5 in hidden layer while in second case neuron size is 1000.\n",
        "\n",
        "Since the accuracy is higher and loss is lower, the results are clearly shows us that increase in neuron size increase accuracy and decrease loss.\n",
        "\n",
        "### Activation Function\n",
        "\n",
        "From the tables in 2 hidden layer part, the 1st table is shows the output with RELU while the other shows the output with tanh and there is certain decrease in accuracy and increase in loss if the function is tanh. So RELU is more efficient for our project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760ccb36",
      "metadata": {
        "id": "760ccb36"
      },
      "source": [
        "### Confusion Matrix for Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff0344d",
      "metadata": {
        "id": "1ff0344d",
        "outputId": "0d61b3c8-4d4f-448a-a5e9-618eca419a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy score:\n",
            "0.17417876241405653\n",
            "Precision score:\n",
            "0.15437851165398744\n",
            "Recall score:\n",
            "0.13376743955706144\n",
            "f1 score:\n",
            "0.11315124985439884\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy score:')\n",
        "print(accuracy_score(Y_test, predictions))\n",
        "print('Precision score:')\n",
        "print(precision_score(Y_test, predictions, average = 'macro'))\n",
        "print('Recall score:')\n",
        "print(recall_score(Y_test, predictions, average = 'macro'))\n",
        "print('f1 score:')\n",
        "print(f1_score(Y_test, predictions, average = 'macro'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "report_and_code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "760ccb36"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}